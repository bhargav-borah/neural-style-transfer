{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Neural Style Transfer","metadata":{}},{"cell_type":"markdown","source":"## 1. Imports","metadata":{}},{"cell_type":"code","source":"import sys\nimport os\n\n# Add the parent directory to path so that we can import the src module\nsys.path.append(os.path.abspath('..'))\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nimport torchvision.transforms as transforms\nfrom PIL import Image\nfrom torchvision.models import vgg19, VGG19_Weights\n\nfrom src.config import CFG\nfrom src.style_transfer import StyleTransfer\nfrom src.model import VGGFeatures\nfrom src.utils import view_image, load_image, save_image, get_white_noise_image","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Configuration","metadata":{}},{"cell_type":"code","source":"print(f\"Using device: {CFG.device}\")\n\nCFG.content_image_path = '../data/content.jpg'\nCFG.style_image_path = '../data/style.jpg'\n\nCFG.num_iterations = 1000\nCFG.print_interval = 100","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Load and View Images","metadata":{}},{"cell_type":"code","source":"content_img = load_image(CFG.content_image_path, CFG)\nstyle_img = load_image(CFG.style_image_path, CFG)\n\nplt.figure(figsize=(10, 5))\n\nplt.subplot(1, 2, 1)\nview_image(content_img, title=\"Content Image\", cfg=CFG)\n\nplt.subplot(1, 2, 2)\nview_image(style_img, title=\"Style Image\", cfg=CFG)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Initialize Style Transfer Model","metadata":{}},{"cell_type":"code","source":"style_transfer = StyleTransfer(\n    content_path=CFG.content_image_path,\n    style_path=CFG.style_image_path,\n    cfg=CFG\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Run Style Transfer and Display Results","metadata":{}},{"cell_type":"code","source":"output_img = style_transfer.run(iterations=1000)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"style_transfer.display_results(output_img)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Save Results","metadata":{}},{"cell_type":"code","source":"output_path = '../outputs/stylized_output.jpg'\nstyle_transfer.save_image(output_img, output_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Experiment with Different Parameters","metadata":{}},{"cell_type":"code","source":"CFG.style_weight = 1e7  \n\nCFG.optimizer = 'lbfgs'\n\nstyle_transfer_experiment = StyleTransfer(\n    content_path=CFG.content_image_path,\n    style_path=CFG.style_image_path,\n    cfg=CFG\n)\n\noutput_img_experiment = style_transfer_experiment.run(iterations=500)\n\nstyle_transfer_experiment.display_results(output_img_experiment)\n\nstyle_transfer_experiment.save_image(output_img_experiment, '../outputs/experimental_output.jpg')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8. Direct Implementation","metadata":{}},{"cell_type":"code","source":"# Implement style transfer directly in this notebook without using the StyleTransfer class\na = load_image(CFG.style_image_path, CFG)  \np = load_image(CFG.content_image_path, CFG) \nx = get_white_noise_image(CFG) \nx.requires_grad_(True) \n\nvgg = VGGFeatures(CFG.device)\n\ncontent_layer = 'conv4_2'\nstyle_layers = ['conv1_1', 'conv2_1', 'conv3_1', 'conv4_1', 'conv5_1']\nweights = [0.2, 0.2, 0.2, 0.2, 0.2]\n\nalpha = 1.0  \nbeta = 1e6   \n\noptimizer = optim.Adam([x], lr=1e-2)\n\nnum_iterations = 1000\nfor iter in range(num_iterations):\n    style_loss = 0\n    for i, layer in enumerate(style_layers):\n        input_feat = vgg.compute_feat_map(x, layer)\n        style_feat = vgg.compute_feat_map(a, layer).detach().clone()\n        \n        input_gram = vgg.compute_gram_matrix(input_feat)\n        style_gram = vgg.compute_gram_matrix(style_feat)\n        \n        size = input_feat.size(0) * input_feat.size(1)\n        style_loss += weights[i] * (F.mse_loss(input_gram, style_gram, reduction='sum') / (4 * size))\n    \n    input_content_feat = vgg.compute_feat_map(x, content_layer)\n    content_feat = vgg.compute_feat_map(p, content_layer).detach().clone()\n    content_loss = 0.5 * F.mse_loss(input_content_feat, content_feat, reduction='sum')\n    \n    loss = alpha * content_loss + beta * style_loss\n    \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    with torch.inference_mode():\n        x.clamp_(0, 1)\n    \n    if (iter + 1) % 100 == 0:\n        print(f'Iteration: {iter + 1} | Loss: {loss.item():.4f}')\n\nplt.figure(figsize=(15, 5))\nplt.subplot(1, 3, 1)\nview_image(x, title=\"Style-Transferred Image\", cfg=CFG)\nplt.subplot(1, 3, 2)\nview_image(p, title=\"Content Image\", cfg=CFG)\nplt.subplot(1, 3, 3)\nview_image(a, title=\"Style Image\", cfg=CFG)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}